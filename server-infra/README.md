# サーバ/インフラを支える技術  
## 用語の整理
* CDN（Content Delivery Network）：コンテンツを配信するためのネットワークシステム。世界中に点在するキャッシュサーバの中から、クライアントにより近いキャッシュサーバを選び配信することにより、性能向上を実現している。  
* LVS（Linux Virtual Server）：Linuxでスケーラブルで可用性の高いシステムを作ろうと目指しているプロジェクト。（慣習的に「Linuxで作ったロードバランサ」を指すこともある）その成果物の一つにLinuxロードバランサのためのIPVSがある。  
* フェイルオーバ（Failover）：冗長化されたシステムにおいて、Activeなノードが停止した際に、自動的にBackupノードに切り替わること。自動ではなく手動で切り替えることは「スイッチオーバ」という。  
* ヘルスチェック（Health Check）：監視対象が正常な状態にあるかどうか確認をすること。例えば、Webサーバに対して、pingが通るか、TCPの80番ポートに接続できるか、HTTPの応答があるか、など。  
* レイテンシ（latency）：データが届くまでの時間を意味する。（cf. スループットは単位時間あたりのデータ転送量）  

## 1章 サーバ/インフラ構築入門  
### 1.1 冗長化の基本
　冗長化とは、障害が発生しても予備の機材でシステムの機能を継続できるようにすること。  
以下のステップが必要である。  
1. 障害を想定する  
2. 障害に備えて予備の機材を準備する  
3. 障害が発生した際に予備の機材に切り替えられる運用体制を整備する  

冗長化されたシステムでは「現用機と予備機の構成を常に同じ状態にしておくことが定石」  
#### コールドスタンバイとホットスタンバイ
ルータなどのネットワーク機器は、運用中に頻繁に設定を変えることもないため、蓄積しなければならないデータもほとんどなく、コールドスタンバイでの運用は現実的な選択肢の一つである。  
一方で、Webサーバの場合、サイトの内容は日々更新されたり、OSのバージョンアップなども避けては通れない。そのため、Webサーバの予備機は常に電源を入れておいて、現用機が内容を更新する際には予備機にも同じ更新がかかるような運用にしたほうがよい。これをホットスタンバイという。  

### 1.2 Webサーバを冗長化する 〜DNSラウンドロビン〜
DNSラウンドロビンとは、DNSを利用して一つのサービスに複数台のサーバを分散させる手法。  
DNSサーバは、同じ名前に複数のレコードが登録されると、問い合わせのたびに異なる結果を返す。この動作を利用することで、複数台のサーバに処理を分散させることができる。  
【問題点】  
- サーバの数だけグローバルアドレスが必要
- 均等に分散されるとは限らない  
WebブラウザなどがDNS問い合わせの結果をキャッシュするため。  
- サーバがダウンしても気づかない  
DNSラウンドロビンはあくまで負荷分散の仕組みであり、冗長化の仕組みではないため、ヘルスチェックやフェイルオーバーを実装する必要がある。

### 1.3 Webサーバを冗長化する 〜IPVSでロードバランサ〜
ロードバランサは、1つのIPアドレスに対するリクエストを複数のサーバへ分散することができる。DNSラウンドロビンと比較して、ロードバランサを利用すると、グローバルアドレスが節約できることや、容易に冗長構成を組めるといった利点がある。  
ロードバランサは、サービス用のグローバルアドレスを持った仮想的なサーバとして動作する。  
#### ロードバランサの種類
ロードバランサには大きく分けてL4スイッチとL7スイッチがある。（単にロードバランサといった場合L4スイッチを指すことが多い）  
L4スイッチはトランスポート層までの情報を解析するので、IPアドレスやポート番号によって分散先のサーバを指定することができる。  
L7スイッチはアプリケーション層までの情報を解析するので、クライアントからリクエストされたURLによって分散先のサーバを指定することができる。  
IPVSに実装されているのはL4スイッチ相当の機能である。  
IPVSの機能が利用可能なソフトウェア  
- ipvsadm  
- keepalived  

### 1.4 ルータやロードバランサの冗長化
#### 冗長化プロトコル（VRRP: Virtual Router Redundancy Protocol）  
ベンダ非依存の冗長化プロトコル。  
keepalivedでもVRRPを利用できるので、ロードバランサをもう1台構築してkeepalivedの設定を追加するだけで冗長化することができる。

## 2章 ワンランク上のサーバ/インフラの構築  
### 2.1 リバースプロキシの導入
ロードバランサとWebサーバの間にリバースプロキシと呼ばれる役割のサーバを挟むことで、より柔軟な負荷分散が可能となる。  
リバースプロキシを利用すると、クライアントからの要求がWebサーバに届く途中の処理に割って入って、さまざまな前後処理を施すことができるようになる。  
以下、リバースプロキシ導入の利点を挙げていく。  
1. HTTPリクエストの内容に応じたシステムの動作の制御  
IPVSはL4なので、クライアントから要求されたHTTPリクエストの内容に応じて処理を振り分けることはできない。  
ここでリバースプロキシがあると、たとえばHTTPリクエストの中からURLを見て、URLが/images/なら画像用のWebサーバ、/newsなら動的コンテンツを作成するWebサーバに処理を振り分けるといった制御が可能になる。

2. システム全体のメモリ使用効率の向上  
動的コンテンツを返却するWebサーバ（APサーバ）では、アプリケーションが利用するプログラムをメモリに常駐させておき、アプリケーション起動時のオーバーヘッドを回避する設計がなされている。そのため、APサーバには大量のメモリが要求される。また、APサーバはクライアント1リクエストに対して1プロセスもしくは1スレッドを割り当てて処理をする方式を取っている。これにより、画像やCSSのような静的コンテンツを返すだけの場合もメモリを大量に消費することになる。  
そこで静的なファイルを返却するWebサーバと、動的コンテンツを生成するAPサーバを別のサーバとして切り分け、静的コンテンツはメモリ消費量の少ないWebサーバが応答し、動的コンテンツのみアプリケーションで応答する構成にすることで、システム全体で見た場合のメモリ使用効率が上がり、同時に処理できるリクエスト数が向上する。この場合のリクエストの振り分けにリバースプロキシを用いるのが有効である。このとき、リバースプロキシ自身もWebサーバであるという特徴を生かして静的コンテンツはリバースプロキシ自身が返却するという構成が一般的である。

3. Webサーバが応答するデータのバッファリングの役割  
#### HTTPのKeep-Alive
あるクライアントが一度に多数のコンテンツを同一のWebサーバから取得する場合、多数のHTTPリクエストごとにサーバとの接続を確立しては切断して‥を繰り返すのは効率がよくない。
HTTPにはKeep-Aliveという仕様があり、これを用いると、最初の1リクエストで確立したサーバとの接続を、そのリクエストが終了した後も切断せずに維持して、続くリクエストでその接続を使い回すことができる。  
リバースプロキシ役のWebサーバは、APサーバと比べプロセスあたりのメモリ使用量がそれほど多くないため、クライアントとリバースプロキシの間のみKeep-Aliveをオンにして、リバースプロキシとAPサーバ側はオフにすることで、クライアントとの接続の維持をリバースプロキシに任せることができる。

4. Apacheモジュールを利用した処理の制御  
リバースプロキシにApacheを採用した場合、そのリバースプロキシにApacheモジュールを組み込んで、HTTPリクエストの前処理/後処理として任意のプログラムを動かすことが可能になる。例えば、mod_dosdetectorはApache2.2用のDos攻撃対策用モジュールで、特定クライアントからの過剰なアクセスを一時的に遮断したりすることができるモジュールである。  
### 2.2 キャッシュサーバの導入
ステートレスなプロトコルは状態を持たないためキャッシュしやすいという特徴がある。そのため、HTTPには、プロトコルのレベルでキャッシュの機能が組み込まれている。ホスト間の関係がどのようなものであれ、両者のやり取りにHTTPプロトコルが使われていれば、その内容はキャッシュすることが可能である。
#### Squidキャッシュサーバ
SquidはHTTP、HTTPS、FTPなどで利用されるオープンソースのキャッシュサーバである。SquidをHTTP通信する2点間の間に配置すると、そのやり取りをキャッシュすることができる。Squidをリバースプロキシとして用いると、サーバサイドのドキュメントをサーバシステム側でSquidにキャッシュさせることができる。  
Squidが内部で持つキャッシュ用のストレージは非常に高速で、かつSquidは大規模なアクセスを少ないリソースで返却できるように設計されている。  
また、Squidは別のSquidとネットワークを介してキャッシュを共有することができる。
#### Squidの限界
HTTPプロトコルをベースにキャッシュするというのはつまり、URLをキーにドキュメントをキャッシュするということでもあるため、一意なURLが与えられているドキュメントは基本的にキャッシュすることが可能である。しかし、ユーザごとに内容が変わるページなど、状態を持つドキュメントのキャッシュはURLをキーにドキュメント全体をキャッシュするHTTPプロトコルレベルでのキャッシュでは難しい。通常、この手の処理はCookieによるセッション管理によって実現されるが、そもそもCookieによるセッション管理はステートレスなプロトコル上に状態、つまりステートフルな通信を持ち込むための仕組みである。このような場合は、キャッシュの粒度を細かくして対応する必要がある。  
あくまでSquidが有効なのは「ページ全体をキャッシュできるようなケース」である。  
#### memcached
memcachedはC言語で書かれた高速なネットワーク対応の分散キャッシュサーバで、ストレージにはOSのメモリを利用している。Squidとは異なり、アプリケーション内部が利用するデータの粒度でキャッシュを管理するキャッシュサーバ。

### 2.3 MySQLのレプリケーション
- MySQLのレプリケーション機能でサポートされているのは、1台のマスタと複数台のスレーブという構成（シングルマスタ、マルチスレーブ）  
- マスタとは、クライアントからの更新と参照の両方のクエリを受け付けるサーバで、スレーブとはクライアントからの更新は受け付けず、データの更新はマスタとの連携のみで行う役割のサーバである。
- SELECT文などの参照系クエリを複数のスレーブに分散させて性能向上を図る、といった構成を作ることもできる。  
- MySQLのレプリケーションは非同期で、SQL文単位で行われる。（MySQL 5.1.5以降は行単位のレプリケーション機能がある）
- レプリケーションの仕組み
 - スレーブではレプリケーションのためにI/OスレッドとSQLスレッドの2つのスレッドが動いている。I/Oスレッドは、マスタから得たデータ（更新ログ）をリレーログと呼ばれるファイルにひたすら記録する。SQLスレッドはリレーログを読み取ってひたすらクエリを実行する。
 - マスタにはバイナリログ、スレーブにはリレーログと呼ばれるファイルが作成される。バイナリログにはデータを更新する処理のみが記録される（参照系のクエリは記録されない）リレーログとは、スレーブのI/0スレッドが、マスタから更新ログを受け取り、スレーブ側に保存したもの。
 - スレーブは、どこまでレプリケーションしたかという情報を覚えている。これらの、マスタのホスト名、ログファイル名、ログファイル中の処理したポイントといった情報のことをポジション情報という

### 2.4 MySQLのスレーブ+内部ロードバランサの活用例
スレーブの活用方法として、スレーブを複数台立てて、参照系のクエリを分散する方法が考えられる。ここで問題になるのはどのようにクエリを分散するかである。
1. アプリケーションで分散
アプリケーション側にスレーブ群のホスト名一覧を持ち、分散先のスレーブサーバを決定するロジックを実装する。このときにスレーブの死活監視を行い、ダウンしているスレーブには分散しないような処理も実装する。
2. 内部ロードバランサで分散
ロードバランサを使うことで、スレーブの死活監視や均一な分散もロードバランサ側に任せられるので、アプリケーション側はスレーブの状態や台数を気にしなくてよくなる。

### 2.5 高速で軽量なストレージサーバの選択
大容量コンテンツを配信するサービスでは、大容量なストレージサーバにファイルを格納し、各WebサーバはNFSマウントをしてファイルを読み出す構成が一般的。  
しかし、以下のような懸念点もある。
- ストレージサーバに障害が発生すると被害が広範囲に及びやすい
- ストレージサーバはスケールアウトしづらいため、ボトルネックになりやすい

## 3章 止まらないインフラを目指すさらなる工夫
### 3.1 DNSサーバの冗長化
- **レゾルバライブラリを利用した冗長化**
さまざまなアプリケーションが名前解決のために利用しているレゾルバライブラリは、`/etc/resolv.conf`を参照して問い合わせ先のDNSサーバを取得している。`/etc/resolv.conf`に複数のDNSサーバを指定すると、順に問い合わせを行なっていく
この手法を用いると、DNSサーバがダウンした場合、タイムアウト（デフォルトは5秒）を待ってから次のサーバへ問い合わせを行う。このため、「サービスの性能は低下するがエラーにならない」という特徴があり、障害の発見を遅らせる要因となる。
- **VRRPを利用した冗長化**
DNSサーバにkeepalivedをインストールして、VIPを割り当てることでVRRPによる冗長化ができる。（Active/Backup構成）
DNSサービスが停止してもフェイルオーバーするようにするために、digコマンドを使って自分自身にDNS問い合わせをし、異常終了したらkeepalivedを停止するなどのヘルスチェックスクリプトを動かしておく必要がある。
- **ロードバランサを利用した冗長化**
上記のActive/Backup構成では1台のDNSサーバしか仕事をしていないため、よりサーバーリソースを活用するためにActive/Active構成にしたい。これはロードバランサを導入して、ロードバランサにVIPを持たせることで達成できる

### 3.2 ストレージサーバの冗長化
ストレージサーバの障害は影響範囲が広いため、RAIDを利用してハードディスク故障によってデータを消失しない構成にするのが一般的。
ストレージサーバを冗長化し、データの整合性を取り続けるのは意外と難しい。2台のサーバにおいてファイル単位で同期したり整合性チェックをすると、ファイルの検索や比較に時間がかかる上に、ハードディスクへの負荷も大きい。
#### DRBD（Distributed Replicated Block Device）
DRBDにはマスタサーバとバックアップサーバがあり（Active/Backup構成）それぞれ、カーネルモジュール（デバイスドライバ）とユーザランドツール（制御プログラム）で構成されている。DRBDのミラーリングでは、ファイル単位でデータを転送するのではなく、ブロックデバイスにに対する更新をリアルタイムで転送している。
DRBDはマスターサーバでトラブルが発生したからといって自動でフェイルオーバーしないため、keepalivedのVRRP機能と組み合わせてNFSサーバを冗長化するなどの工夫が必要。

### 3.3 ネットワークの冗長化
物理的なネットワーク（L1=物理層）やEthernetのレベルでの通信（L2=データリンク層）を冗長化しておけば故障を回避するだけでなく、システムをメンテナンスする上でも非常に有効である。
L1/L2の構成要素で故障するものとして、次のものが考えられる
- LANケーブル
- NIC（ネットワークカード）
- ネットワークスイッチのポート
- ネットワークスイッチ
#### サーバとスイッチ間の接続の冗長化
LinuxにはBondingドライバというネットワークドライバが用意されている。Bondingドライバは複数の物理的なネットワークカード（物理NIC）をまとめて、1つの論理的なネットワークカード（論理NIC）として扱えるようにする。論理NICを通じた通信は、Bondingドライバが配下の物理NICに割り振り、また配下の物理NICが故障していないかチェックして、故障していればその物理NICは使わないようにしてくれる。
#### スイッチの冗長化
スイッチ故障を回避するためには、スイッチを複数台用意して、Bondingドライバ配下の物理NICをそれぞれ別のスイッチに接続する。
また、RSTP（Rapid Spanning Tree Protocol）を使えば、スイッチの冗長化をBondingドライバに頼らず実現できる。RSTPは各スイッチが協調してネットワーク上にできたループを検出し、自動的に冗長な接続を遮断するためのデータリンク層のプロトコル

### 3.4 VLANの導入
ネットワークの構成変更などで行われる物理作業のなかには、VLAN（Virtual LAN）の機能を利用すれば、物理作業をともなわずに対応が可能であるものも多くある。
VLANとは物理的な構成ではなく、ネットワーク機器やサーバの設定で「論理的」にネットワークを分割して構成する技術。
VLANを導入するメリットは主に以下の2つ
- 1台のスイッチで複数のセグメントを管理できる
- サーバ追加/置き換え、故障時の代替機による復旧が容易になる

## 4章 性能向上、チューニング
### 4.1 Linux単一ホストの負荷を見極める
- Webアプリケーションの負荷分散は、多くの場合「ディスクI/Oの分散と軽減」作業。OSはI/Oを軽減するためにキャッシュの仕組みを内包しており、このキャッシュが有効に働くようにシステムを組むのが、I/O分散のコツである。
- **推測するな、計測せよ**
- **OS知らずして負荷分散を語ることなかれ**
#### 負荷とは何か
負荷は大きく分けて以下の2つに分類されます。
- CPU負荷
- I/O負荷
`top`コマンドの出力には下記のようにload averave（ロードアベレージ）という数字が含まれています。
```
load average: 0.70 0.66 0.59
```
これは左から順に1分、5分、15分の間に、単位時間あたり待たされたタスクの数、つまり平均的にどの程度のタスクが待ち状態にあったかを表す数字。ロードアベレージが高い状況は、負荷が高い状況といってよいが、これを見ただけではCPU負荷が高いのか、I/O負荷が高いのかは判断できない。
結局のところ、負荷というのは、複数のタスクによるサーバリソースの奪い合いの結果に生じる待ち時間のことである。Linuxでは、プロセススケジューラと呼ばれるプログラムがマルチタスク制御における実行するタスクの優先度を決め、タスクを待たせたり再開させるという仕事を担っている。したがって、プロセススケジューラの仕組みを見ていくと負荷の正体が見えてくる。
